{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2886aad0-80ce-47fe-91ee-666abbdfe989",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# LLM in Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31195021-663f-4b5a-97d9-6254948057db",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## First steps with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d3f0e-ac24-495f-b331-d591b6f9dc6e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip uninstall llama-cpp-python\n",
    "# build on an old Intel MBP where Metal is not supported\n",
    "!FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aca8e8-a043-47f1-a3e3-70bbc962d4bf",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "model_name=\"mistral-7b-instruct-v0.1.Q5_K_M\"\n",
    "model_name=\"TinyLlama-1.1B-Chat-v0.3\"\n",
    "model_ame=\"ggml-model-Q4_K\"\n",
    "\n",
    "\n",
    "model = Llama(model_path=f\"/Users/alleon_g/.cache/llama.cpp/models/{model_name}.gguf\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a569dd-107e-42f2-ac0e-3f099bf473dc",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer = model('Please answer in one sentence to this question: What is a Large Language Model?', stop=[\"<|im_start|>\"]) # , \"\\n\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c324bc-adbd-4e08-ad2a-6d0c4a857fd4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb282c-9e0a-42a6-87b1-f7e4bce71989",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(answer['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c05b0-5b93-422a-8394-1fcef2ec0f8e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929216d4-c829-46eb-972d-b7b99733d8b3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- Intro to LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6a3d5b-a593-422c-a131-071bb8d445ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- The wide range of abilitiers of LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9556ee67-4c49-4735-9b4f-972a2f71da97",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- Training and Deploying LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4dc185-ca13-451c-829f-ef4e6c4fc4cc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- Getting Commercial Value from LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbda87f-22b1-4386-baec-23ab0ae4c6bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Quick history of LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b40d48-eb51-4c9f-9e9b-05a8ee28220b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "In the vast landscape of technological advancement, the journey of language models stands out as a testament to human ingenuity. Much like the epochs of human evolution, where our ancestors transcended through distinct ages defined by tools and discoveries, language processing models too have charted an illustrious journey, each phase building upon the last, refining the art and science of understanding and generating human language.\n",
    "\n",
    "The Stone Age of language models saw the rudimentary yet pioneering techniques like Bag of Words and TF-IDF. These methods, in their simplicity, were akin to the early tools fashioned by our forebears - basic, yet fundamental in the march towards progress. The Bronze Age brought sophistication, introducing us to the magic of embeddings via methods like Word2Vec and Doc2Vec. This era celebrated the enchanting algebra of words, with the classic equation \"king - man + woman = queen\" encapsulating the groundbreaking progress.\n",
    "\n",
    "The Iron Age was a realm of complexity and nuance. RNNs and LSTMs came to the forefront, harnessing the temporal essence of language. Yet, as metals are tempered to achieve sharper edges, the world of deep learning bore witness to a pivotal mechanism - attention. Just as iron tools were sharper and more durable than bronze, this newfound technique started to show potential beyond what was previously imagined.\n",
    "\n",
    "The Industrial Revolution, a period of rapid innovation and progress in human history, finds its parallel in the world of language models with the advent of Transformers. A period marked by the dazzling pace of discovery, this era saw models like GPT, BERT, and others rise to prominence. But as with any revolution, refinement is inevitable. The models were fine-tuned, and techniques such as Reinforcement Learning from Human Feedback (RLHF) were developed, echoing the relentless quest for perfection that marked the historical Industrial Revolution.\n",
    "\n",
    "Join us as we embark on this enlightening journey, drawing parallels between human evolution and the advancement of language models, celebrating the monumental strides made in a relatively short span of time. This chapter promises a symphony of past discoveries and a harbinger of the future's potential.\n",
    "\n",
    "### **The Stone Age of Language Models: Bag of Words and TF-IDF**\n",
    "\n",
    "The dawn of language processing was characterized by simplicity, foundational discoveries, and the establishment of fundamental techniques that paved the way for more complex developments. Two of the most fundamental techniques of this era in the domain of Natural Language Processing (NLP) were the Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF).\n",
    "\n",
    "**1. Bag of Words (BoW):**\n",
    "The Bag of Words model, as its name suggests, represents a text as an unordered set of words. Essentially, it involves tokenizing the text into words and counting the frequency of each word. This method disregards grammar, order, and context, focusing solely on word occurrence. In essence,it is  straightforward and foundational. While BoW is intuitive and easy to implement, its simplicity also becomes its limitation, as it lacks the ability to capture semantic relationships between words and the context in which they appear.\n",
    "\n",
    "**2. Term Frequency-Inverse Document Frequency (TF-IDF):**\n",
    "While the BoW model laid the groundwork, TF-IDF built upon it by introducing a measure of importance to each term in a document relative to a collection of documents or corpus. The core idea behind TF-IDF is that words that appear frequently in one document but not in many documents across the corpus likely hold more significance. This technique balances the frequency of a term (Term Frequency) against its rarity across multiple documents (Inverse Document Frequency), providing a weight that can be used to rank terms' relevance within a document[^1^].\n",
    "\n",
    "Despite the simplicity of these models, they served as crucial stepping stones. Both BoW and TF-IDF became cornerstones for many early information retrieval systems and remain relevant for various applications today.\n",
    "\n",
    "However, as we transitioned from this metaphorical 'Stone Age', it became evident that capturing the complexity and nuance of human language required more than just term frequencies and weights. The era demanded models that could understand context, semantic relationships, and the subtleties that make language rich and intricate. This realization led to the subsequent developments and the ushering in of the Bronze Age of NLP.\n",
    "\n",
    "### **The Bronze Age of Language Models: Word Embeddings and the Semantics of Language**\n",
    "\n",
    "As human civilization progressed into the Bronze Age, tools became more refined, cultures grew richer, and the limits of the known world expanded. Similarly, in the landscape of NLP, the realization that raw counts and frequencies weren't capturing the essence of language gave rise to more sophisticated methods. Enter the world of word embeddings, where words were represented as dense vectors capturing the semantics and contextual relationships between them.\n",
    "\n",
    "**1. Word2Vec:**\n",
    "Developed by [Mikolov et al.](https://arxiv.org/abs/1301.3781) at Google, Word2Vec became one of the most popular methods for learning word embeddings. Instead of sparse vectors used in BoW and TF-IDF, Word2Vec represented words as dense vectors, typically with several hundred dimensions[^1^]. These vectors were learned using neural networks by predicting a word given its context (Continuous Bag of Words approach) or predicting context words given a target word (Skip-Gram approach). The underlying principle was simple: words that appear in similar contexts tend to have similar meanings. The result was groundbreaking; the word vectors captured a wide range of semantic and syntactic relationships. For instance, the model could capture relationships such as \"king\" - \"man\" + \"woman\" ≈ \"queen\", showcasing the [algebra of semantics](https://aclanthology.org/N13-1090.pdf).\n",
    "\n",
    "**2. Doc2Vec:**\n",
    "While Word2Vec was revolutionary in representing individual words, there was a need to capture representations for larger text units like sentences or documents. [Le and Mikolov](https://arxiv.org/abs/1405.4053) introduced Doc2Vec (or Paragraph Vector) to address this. Doc2Vec extends the Word2Vec algorithm to capture document-level embeddings, encapsulating the semantic meaning of varied-length pieces of texts.\n",
    "\n",
    "Visualize word embeddings as a vast landscape where each word is a location. Words with similar meanings are closer to each other, while unrelated words are distant. This spatial representation provides a bird's-eye view of the vast semantics of language, akin to observing settlements and cultures from a high vantage point in the Bronze Age.\n",
    "\n",
    "These embeddings became the foundation for a plethora of NLP tasks, from sentiment analysis to machine translation. They provided the crucial leap from mere word counts to understanding the semantics of language.\n",
    "\n",
    "As the Bronze Age set the stage for a subsequent era of enhanced tools and techniques, word embeddings prepared the ground for models that could further dive into the intricacies of language, leading us to the Iron Age of NLP.\n",
    "\n",
    "### **The Iron Age of Language Models: Recurrent Networks, Attention, and the Intricacies of Temporal Dependencies**\n",
    "\n",
    "Progressing from the Bronze Age, the Iron Age in human history marked a period of rapid advancements in tools and methods, enabling the construction of stronger, more durable, and complex artifacts. Similarly, in the world of NLP, as the limitations of mere word embeddings became evident, there was a transition to more intricate models that could handle the temporal and sequential nature of language. This led to the rise of Recurrent Neural Networks (RNNs), Long Short-Term Memory networks (LSTMs), and the pivotal attention mechanism.\n",
    "\n",
    "**1. Recurrent Neural Networks (RNNs):**\n",
    "Traditional neural networks, while potent, struggled with sequential data. They lacked memory of previous inputs in a sequence, which is crucial for understanding languages where words and their meanings often depend on preceding words. [RNNs](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf) addressed this by having loops that allowed information persistence. In essence, RNNs could remember past information and were aptly suited for tasks like language modeling and machine translation.\n",
    "\n",
    "However, RNNs were not without flaws. They struggled with long-term dependencies due to issues like vanishing and exploding gradients.\n",
    "\n",
    "**2. Long Short-Term Memory Networks (LSTMs):**\n",
    " In NLP, the meaning of a word often depends on the preceding words or the overall context. Traditional neural networks couldn't effectively capture this temporal relationship. LSTMs, with their ability to remember past information, became aptly suited for tasks like language modeling, sentiment analysis, and machine translation. Their architecture, consisting of input, forget, and output gates, allows them to decide what information to retain or discard, thereby preserving the context over long sequences. A significant NLP application of LSTMs came in the form of sequence-to-sequence models. These models, often used in tasks like machine translation, utilized an LSTM-based encoder to read and encode the source sentence and an LSTM-based decoder to generate the target sentence[^1^](https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf). In this landscape, LSTMs provided a significant leap, enabling models to dive deeper into the intricacies and subtleties of human language.\n",
    "\n",
    "**3. Attention Mechanism:**\n",
    "Despite the capabilities of LSTMs, there was still room for improvement. Enter the [attention mechanism](https://arxiv.org/abs/1409.0473). It allowed models to focus on specific parts of the input sequence when producing an output, much like how humans pay attention to specific words when listening or reading. Initially used to boost performance in machine translation tasks, attention quickly became a cornerstone for various NLP applications. \n",
    "\n",
    "Just as iron reshaped civilizations by enabling the construction of advanced machinery, RNNs, LSTMs, and the attention mechanism redefined the landscape of NLP, setting the stage for the next revolutionary era—the Industrial Revolution of language models.\n",
    "\n",
    "### **The Industrial Revolution of Language Models: Transformers and the Power of Parallel Processing**\n",
    "\n",
    "As the Industrial Revolution marked a radical transformation of society through mechanization and the development of new technologies, a similar paradigm shift occurred in the world of Natural Language Processing (NLP) with the advent of transformer architectures. The transformer era was characterized by parallel processing, scalability, and models that displayed an unprecedented ability to understand and generate human-like text.\n",
    "\n",
    "**1. The Transformer Architecture:**\n",
    "Introduced by [Vaswani et al]((https://arxiv.org/abs/1706.03762)). in 2017, the transformer architecture revolutionized NLP by discarding recurrence and focusing entirely on attention mechanisms, enabling parallelization. This allowed for the training of significantly larger models, yielding improved results across a multitude of NLP tasks.\n",
    "\n",
    "**2. BERT and Pre-trained Language Models:**\n",
    "Building on the transformer's foundation, [BERT](https://arxiv.org/abs/1810.04805) (Bidirectional Encoder Representations from Transformers) by Devlin et al. showcased the power of pre-training and fine-tuning. By pre-training on a vast corpus and fine-tuning on specific tasks, BERT set new state-of-the-art results on eleven natural language processing tasks, emphasizing the benefits of general-purpose language representation.\n",
    "\n",
    "**3. GPT and Generative Modeling:**\n",
    "OpenAI's Generative Pre-trained Transformer ([GPT]((https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf))) highlighted the capabilities of transformers in generating coherent, diverse, and contextually relevant sentences. The subsequent versions, especially GPT-3, stunned the community with its ability to draft essays, answer questions, and even write poetry.\n",
    "\n",
    "**4. Customization and Fine-tuning with Reinforcement Learning:**\n",
    "Transformers' scalable nature allowed researchers to explore the intersection of reinforcement learning and NLP. Techniques such as Supervised Fine-Tuning ([SFT]()) and Reinforcement Learning from Human Feedback ([RLHF](https://arxiv.org/abs/1706.03741)) were introduced to fine-tune models, making them more aligned with human values and specific application needs. \n",
    "\n",
    "Envision this era as an industrial factory, where the assembly line (parallel processing) efficiently processes vast amounts of data, producing refined, high-quality products (human-like text and understanding). Just as the steam engine, telegraph, and railroads accelerated society's progress during the Industrial Revolution, transformers and their progenies are pushing the boundaries of what's possible in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b51b63-25f4-4c9c-89f8-d77b58b6fedd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Prehistoric\n",
    "  - bag of words [1954]\n",
    "  - tf-idf [1972]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3027d57c-5972-4029-b972-ed3856052e80",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Bronze Age\n",
    "  - word2vec [2013], doc2vec\n",
    "  - king - man + woman = queen\n",
    "  - Embeddings -> Outcome: LSTMs, RNNs, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7032847a-808e-44ec-aadf-3c319e06f8dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Iron Age\n",
    "  - attention mechanism\n",
    "  - self-supervised learning\n",
    "  - power-law scalings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb9037-5d53-49ef-99c3-600f32bba840",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Industrial Revolution\n",
    "  - Reinforcement Learning with Human Feedback (RLHF)\n",
    "  - Model alignment\n",
    "  - InstructGPT [2022](https://arxiv.org/abs/2203.02155), ChatGPT [2022](https://help.openai.com/en/articles/6825453-chatgpt-release-notes), GPT4 [2023](https://arxiv.org/abs/2303.08774), Anthropic, Cohere, Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fec242e-026d-40cb-9ec0-72fb1de5e068",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fe399c-32bd-4ec3-b295-ede2dc363495",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "* Attention was used in the _Bronze age_\n",
    "* But Transformers kickstarted the _Iron Age_\n",
    "    - demonstrating that **Attention is all you need** [2017](https://arxiv.org/abs/1706.03762)\n",
    "    - avoiding recurrence and/or convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423be147-19ce-4038-9925-66912e2f82dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "![Transformers](./Sequence_to_Sequence.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bf1ede-b72a-4155-9ddf-4223b73c8545",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Transformers in a Nutshell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e671419-70a6-46d1-8b78-7621504d2a78",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th colspan=\"2\"></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td><img width=\"500\" src=\"transformers.png\"/></td>\n",
    "      <td>\n",
    "          <ul>\n",
    "              <li style=\"font-size: 20pt;\">The input of the transformer model is a prompt that needs  to be embedded</li>\n",
    "              <p></p>\n",
    "              <li style=\"font-size: 20pt;\">The block is the main source of complexity. Each block contains a masked multi-head attention, a feed forward network and several layers of normalization</li>\n",
    "              <p></p>\n",
    "              <li style=\"font-size: 20pt;\">The output is fed through one more linear layer to obtain the final output</li>\n",
    "          </ul>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1015c2-d5c5-43b4-a168-1e68ca4c1b66",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Tokenization in a Nutshell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6b018f1-a879-4aaa-a139-28690f8bebcc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f432b1ce-73db-4631-8aed-d72d3023d9a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alleon_g/.pyenv/versions/3.11/envs/llm_slides/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b43d0da-e3e0-4f94-808f-4a684b92d39a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8aaf0a03-f5fc-4e2a-b294-352e521d6747",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The capital of Belgium is Brussels. The capital of France is Paris.'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"The capital of Belgium is Brussels. The capital of France is\", max_new_tokens=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da678e8e-cccd-41f8-9436-e4e9406949bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74c7a5f5-7ef8-4d84-9a1f-ca5e14fb3f0d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Megatron' in tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a0f97fa-924f-4fcb-b621-70989869aa99",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42672, 23484, 10408, 691, 2241]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Megatron loves only himself')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f040edd1-94cf-49a5-9edd-3bf785103198",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Meg', 'atron', 'Ġloves', 'Ġonly', 'Ġhimself']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer.encode('Megatron loves only himself'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5dd79b5-a253-43a1-a72a-82a2d3f21b25",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Ġloves' in tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc66f5c5-ba60-4dac-9498-ba9eebdaa8e4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded_input = tokenizer('Megatron loves only himself')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbb9265d-fffd-4cde-ac75-51ff847fccf3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98ffa9d-dbd1-4b67-973d-49c12b8623a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "Autoregressive models                                 |  Autoencoding models                                  |\n",
    "------------------------------------------------------|-------------------------------------------------------| \n",
    "Predict tokens based on past context i.e.             | Predict tokens based on past and future context i.e.  |\n",
    "  **'Gérard loves __.'**                              |   **'Gérard loves __ pizas.'**                        |\n",
    "  Good for Natural Language Generation                |  Good for Natural Language Understanding              |\n",
    "  GPT architectures                                   |  BERT architectures                                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22327c45-29ad-46c5-b3ee-e00d191f8a10",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d49fefd-f082-4601-8d18-ef25570ca6cb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- Number of parameters > 100M\n",
    "    - Largest is [WuDao](https://www.baai.ac.cn/english.html#Research) with 1.75T parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dec2654-8940-48dd-adce-dd7125b4fb4f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Don't necessarily rely on the Transformer architecture\n",
    "    * but maby do - some are hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57940624-0b91-4acf-be3b-271ee24d3f12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Trained on very large corpora\n",
    "    * ThePile [2020](https://arxiv.org/abs/2101.00027) is a 825 GiB dataset that consists of 22 smaller, high-quality datasets combined together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f777eb67-9240-46cf-a1f1-50af815e04c7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "* Can be finetuned for specific usages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21809b1-e1ee-4177-b1eb-57a7d23c7a40",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Bigger LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffbb0f1-e5ce-47f2-b436-d86835b7cc04",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "![LLMs](2023-Alan-D-Thompson-AI-Bubbles-Optimal-Rev-5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec9bec0-fc62-4732-ae9d-621d078ffd15",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
